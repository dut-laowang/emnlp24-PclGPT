## ğŸ“ This paper has been accepted in EMNLP 2024 (Findings)
Paper link: [Link to paper](#)
# PclGPT
PclGPT is a bilingual large language model group (LLM) based on ChatGLM-3 and LLaMA-2, divided into two versions according to the training language: PclGPT-CN (based on ChatGLM) and PclGPT-EN (based on LLaMA). Built upon these foundational models, PclGPT has undergone both pre-training and supervised fine-tuning (SFT) to detect patronizing and condescending language (PCL) and other offensive speech. The maximum supported context length for the model is 4096 tokens.

PclGPTæ˜¯ä¸€æ¬¾åŸºäº ChatGLM-3 å’Œ LLaMA-2 çš„åŒè¯­è¨€å¤§å‹è¯­è¨€æ¨¡å‹ç»„ (LLM)ï¼Œæ ¹æ®è®­ç»ƒè¯­è¨€åˆ†ä¸ºPclGPT-CNï¼ˆåŸºäºChatGLMï¼‰ å’ŒPclGPT-ENï¼ˆåŸºäºLLaMAï¼‰ã€‚åœ¨åŸºåº§çš„åŸºç¡€ä¸Šï¼ŒPclGPTç»¼åˆè¿›è¡Œäº†é¢„è®­ç»ƒå’Œç›‘ç£å¼å¾®è°ƒ (SFT), ç”¨äºè¿›è¡Œ**å±…é«˜ä¸´ä¸‹è¨€è®º**(Patronizing and Condescending Language, PCL)å’Œå…¶ä»–æ”»å‡»æ€§è¨€è®ºçš„æ£€æµ‹ã€‚æ¨¡å‹æ”¯æŒçš„æœ€å¤§ä¸Šä¸‹æ–‡ä¸º4096ã€‚

# è®­ç»ƒè¿‡ç¨‹
![æˆ‘ä»¬é€šè¿‡æ„å»ºPcl-PTé¢„è®­ç»ƒæ•°æ®é›†, Pcl-SFTç›‘ç£å¾®è°ƒæ•°æ®é›†ä»¥åº”ç”¨äºé¢„è®­ç»ƒ/ç›‘ç£å¾®è°ƒè¿‡ç¨‹ã€‚å…·ä½“çš„æ„å»ºå’Œè®­ç»ƒæµç¨‹å¦‚ä¸‹å›¾æ‰€ç¤ºã€‚](https://github.com/dut-laowang/PclGPT/blob/main/figures/framework.PNG)

# æµ‹è¯•ç»“æœ
æˆ‘ä»¬åœ¨ä¸¤ä¸ªå±…é«˜ä¸´ä¸‹æ£€æµ‹å…¬å¼€è‹±æ–‡æ•°æ®é›†ï¼ˆTalkdown, Don't Patronize Me) å’Œä¸€ä¸ªä¸­æ–‡æ•°æ®é›†ï¼ˆCPCLï¼‰ä¸Šè¯„ä¼°äº†æ¨¡å‹ç»„çš„æ£€æµ‹æ€§èƒ½ã€‚

æ€§èƒ½æŒ‡æ ‡é‡‡ç”¨ Macro è®¡ç®—çš„ Precisionã€Recallã€F1-score

## è‹±æ–‡
åœ¨PclGPT-EN è‹±è¯­ç»„çš„æ£€æµ‹ä»»åŠ¡ä¸­çš„ç»“æœä¸º
<table>
  <thead>
    <tr>
      <th rowspan="2">LM</th>
      <th rowspan="2">Model</th>
      <th colspan="3">DPM</th>
      <th colspan="3">TD</th>
    </tr>
    <tr>
      <th>P</th><th>R</th><th>F1</th>
      <th>P</th><th>R</th><th>F1</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td rowspan="3">PLMs</td>
      <td>RoBERTa</td><td>76.3</td><td>78.7</td><td>77.4</td><td>88.4</td><td>86.7</td><td>86.5</td>
    </tr>
    <tr>
      <td>RoBERTa-L</td><td>80.2</td><td>74.9</td><td>77.2</td><td>88.1</td><td>86.0</td><td>85.9</td>
    </tr>
    <tr>
      <td>M-BERT</td><td>69.2</td><td>76.0</td><td>71.8</td><td>87.6</td><td>87.4</td><td>87.4</td>
    </tr>
    <tr>
      <td rowspan="3">Base-LLMs</td>
      <td>ChatGPT</td><td>50.8</td><td>52.3</td><td>46.9</td><td>59.2</td><td>58.1</td><td>56.7</td>
    </tr>
    <tr>
      <td>GPT-4.0</td><td>51.5</td><td>57.5</td><td>54.3</td><td>60.8</td><td>60.3</td><td>60.5</td>
    </tr>
    <tr>
      <td>Claude-3</td><td>52.3</td><td>52.5</td><td>52.3</td><td>61.6</td><td>64.1</td><td>63.2</td>
    </tr>
    <tr>
      <td rowspan="2">Base-LLMs</td>
      <td>LLama-2-7B</td><td>50.9</td><td>52.6</td><td>51.4</td><td>49.9</td><td>49.7</td><td>49.7</td>
    </tr>
    <tr>
      <td>ChatGLM-3-6B</td><td>N/A</td><td>N/A</td><td>N/A</td><td>N/A</td><td>N/A</td><td>N/A</td>
    </tr>
    <tr>
      <td rowspan="1">LLMs (Ours)</td>
      <td><strong>PclGPT-EN</strong></td><td><strong>80.4</strong></td><td><strong>81.8</strong></td><td><strong>81.1</strong></td><td><strong>89.9</strong></td><td><strong>89.0</strong></td><td><strong>88.9</strong></td>
    </tr>
  </tbody>
</table>


## ä¸­æ–‡
åœ¨PclGPT-CN ä¸­æ–‡ç»„çš„æ£€æµ‹ä»»åŠ¡ä¸­çš„ç»“æœä¸º
<table>
  <thead>
    <tr>
      <th rowspan="2">LM</th>
      <th rowspan="2">Model</th>
      <th colspan="3">CPCL (CN)</th>
    </tr>
    <tr>
      <th>P</th><th>R</th><th>F1</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td rowspan="4">PLMs</td>
      <td>RoBERTa</td><td>61.2</td><td>61.3</td><td>61.3</td>
    </tr>
    <tr>
      <td>RoBERTa-L</td><td>62.5</td><td>61.6</td><td>62.0</td>
    </tr>
    <tr>
      <td>Chinese-BERT</td><td>66.6</td><td>71.0</td><td>67.3</td>
    </tr>
    <tr>
      <td>M-BERT</td><td>65.8</td><td>67.8</td><td>66.6</td>
    </tr>
    <tr>
      <td rowspan="3">Base-LLMs</td>
      <td>ChatGPT</td><td>53.1</td><td>54.2</td><td>53.6</td>
    </tr>
    <tr>
      <td>GPT-4.0</td><td>55.4</td><td>56.3</td><td>55.7</td>
    </tr>
    <tr>
      <td>Claude-3</td><td>57.2</td><td>57.7</td><td>57.3</td>
    </tr>
    <tr>
      <td rowspan="2">Base-LLMs</td>
      <td>LLama-2-7B</td><td>45.2</td><td>47.5</td><td>46.3</td>
    </tr>
    <tr>
      <td>ChatGLM-3-6B</td><td>51.9</td><td>50.2</td><td>51.0</td>
    </tr>
    <tr>
      <td rowspan="1">LLMs (Ours)</td>
      <td><strong>PclGPT-CN</strong></td><td><strong>69.1</strong></td><td><strong>72.0</strong></td><td><strong>70.2</strong></td>
    </tr>
  </tbody>
</table>


# æƒé‡å’Œä¸‹è½½
æˆ‘ä»¬åœ¨Hugging Faceä¸Šå‘å¸ƒäº†æˆ‘ä»¬çš„ PclGPT-CN å’Œ PclGPT-EN çš„ 1.0ç‰ˆæœ¬æƒé‡ 

PclGPT-EN: [https://huggingface.co/DUTIR-Wang/PclGPT-EN](https://huggingface.co/DUTIR-Wang/PclGPT-EN)

PclGPT-CN: [https://huggingface.co/DUTIR-Wang/PclGPT-CN](https://huggingface.co/DUTIR-Wang/PclGPT-CN)

# æ¨ç†
ä¸‹è½½æƒé‡åï¼Œä½¿ç”¨ä»¥ä¸‹ä»£ç è¿›è¡Œå•æ¡æ¨ç†

```python
from transformers import AutoTokenizer, AutoModel

tokenizer = AutoTokenizer.from_pretrained("/root/autodl-tmp/EMNLP-CCPC-LORA-M2", trust_remote_code=True)
model = AutoModel.from_pretrained("/root/autodl-tmp/EMNLP-CCPC-LORA-M2", trust_remote_code=True).half().cuda()

def generate_response():
    # æ ·ä¾‹æ–‡æœ¬
    sample_text = "åŒºåŒºæ‰«åœ°çš„ï¼Œä½ å¯çœŸæ˜¯å‰å®³åäº†"
    
    instruction = (
        "å‡å®šä½ æ˜¯ä¸€åè¯­è¨€å­¦å®¶ï¼Œæ£€æµ‹å±…é«˜ä¸´ä¸‹è¨€è®ºã€‚å±…é«˜ä¸´ä¸‹è¨€è®ºæ˜¯ä¼˜åŠ¿åœ°ä½ç¾¤ä½“é’ˆå¯¹å¼±åŠ¿ç¾¤ä½“çš„ä¼˜è¶Šè¨€è®ºï¼Œ"
        "è¯­è¨€æ”»å‡»æ€§è¾ƒå¼±ï¼Œä½†å¾€å¾€å¤¹æ‚ç€è®½åˆºï¼Œåˆ»æ¿å°è±¡ï¼Œä¼šå¯¹å¼±åŠ¿ç¾¤ä½“é€ æˆä¼¤å®³ã€‚"
        "å±…é«˜ä¸´ä¸‹æ ¹æ®è¯­æ°”ç¨‹åº¦åˆ†ä¸º Açº§ï¼šéå±…é«˜ä¸´ä¸‹ï¼ˆæ­£å¸¸éæ­§è§†ã€éå†·æš´åŠ›è¨€è®ºï¼‰ï¼Œ"
        "Bçº§ï¼šè½»å¾®å±…é«˜ä¸´ä¸‹ï¼ˆè¯­æ°”ç•¥å¸¦é¼“åŠ±æˆ–æ€œæ‚¯ï¼‰ï¼ŒCçº§ï¼šä¸­ç­‰å±…é«˜ä¸´ä¸‹ï¼ˆè¯´è¯äººè¾ƒä¸ºå®¢è§‚é™ˆè¿°ï¼Œä½†è¯­æ°”å¸¦æœ‰æ­§è§†ï¼‰ï¼Œ"
        "Dçº§ï¼šä¸¥é‡å±…é«˜ä¸´ä¸‹ï¼ˆè¯´è¯äººè¯­æ°”è½»è”‘ï¼Œä¸¥é‡æ­§è§†å¼±åŠ¿ç¾¤ä½“ï¼‰ã€‚"
        "æ¥ä¸‹æ¥å°†ç»™ä½ ä¸€æ®µæ–‡æœ¬ï¼Œæ ¹æ®ä¸Šè¿°è§„åˆ™ï¼Œä½ è´Ÿè´£åˆ¤æ–­è¯¥æ–‡æœ¬å±äºï¼ˆA/B/C/Dçº§ï¼‰çš„å“ªä¸€çº§ï¼Œå¹¶åªå›ç­”é€‰é¡¹ã€‚"
        "-> æ–‡æœ¬ï¼š({})"
    ).format(sample_text)

    # Tokenizer å’Œæ¨¡å‹æ¨ç†
    inputs = tokenizer(instruction, return_tensors="pt").to("cuda")
    outputs = model.generate(**inputs, max_length=1024)
    output_text = tokenizer.decode(outputs[0], skip_special_tokens=True)
    model_output = extract_option(output_text)
    return classify_output(model_output)

def extract_option(output_text):
    options = ['A', 'B', 'C', 'D']
    for char in reversed(output_text.strip()):
        if char in options:
            return char
    return "æ— æ³•è¯†åˆ«çš„è¾“å‡º" 

def classify_output(model_output):
    # æ ¹æ®æ¨¡å‹è¾“å‡ºçš„é€‰é¡¹è¿”å›ç›¸åº”çš„è§£é‡Š
    if model_output == "A":
        return "åˆ¤æ–­ä¸ºAçº§ï¼šéå±…é«˜ä¸´ä¸‹"
    elif model_output == "B":
        return "åˆ¤æ–­ä¸ºBçº§ï¼šè½»å¾®å±…é«˜ä¸´ä¸‹"
    elif model_output == "C":
        return "åˆ¤æ–­ä¸ºCçº§ï¼šä¸­ç­‰å±…é«˜ä¸´ä¸‹"
    elif model_output == "D":
        return "åˆ¤æ–­ä¸ºDçº§ï¼šä¸¥é‡å±…é«˜ä¸´ä¸‹"
    else:
        return "æ— æ³•è¯†åˆ«çš„è¾“å‡ºï¼Œè¯·æ£€æŸ¥è¾“å…¥æˆ–æ¨¡å‹è¾“å‡º"

response = generate_response()
print(response)
```
å¾—åˆ°çš„è¾“å‡ºä¸º
```
"åˆ¤æ–­ä¸ºDçº§ï¼šä¸¥é‡å±…é«˜ä¸´ä¸‹"
```
# å¼•ç”¨
å¦‚æœä½ è®¡åˆ’åº”ç”¨æˆ–æ‰©å±•æˆ‘ä»¬çš„å·¥ä½œï¼Œè¯·å¼•ç”¨ä»¥ä¸‹è®ºæ–‡
